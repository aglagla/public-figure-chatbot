# Database

DATABASE_URL=postgresql+psycopg2://postgres:postgres@db:5432/chatbot

# Backend

BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
BACKEND_URL=http://backend:8000

# LLM (OpenAI-compatible)
# Switch these based on profile:
# - CPU profile: http://llm_cpu:8000/v1
# - GPU profile: http://llm:8000/v1

LLM_BASE_URL=http://llm_cpu:8000/v1
LLM_API_KEY=not-needed
LLM_MODEL=default
LLM_PORT=8001

# llama.cpp (CPU LLM)
# (watch carefully directory names and file names)
LLM_MODELS_HOST_DIR=/models/llm/llama-3.1-8b-instruct-gguf/
LLM_GGUF_FILENAME=Meta-Llama-3.1-8B-Instruct-Q5_K_S.gguf
LLM_CPU_THREADS=8
LLM_CPU_BATCH_SIZE=256

# vLLM (GPU LLM) â€” if you use GPU profile, point LLM_MODELS_HOST_DIR to a HF model directory
# LLM_MODELS_HOST_DIR=/srv/models/llm/Llama-3.1-8B-Instruct

# Embeddings (TEI)
# - CPU profile: http://embeddings_cpu:80
# - GPU profile: http://embeddings:80
# ---------------------------
EMBEDDINGS_BASE_URL=http://embeddings_cpu:80
EMBED_PORT=8080

# EXACT host path to the embedding model folder (must contain config.json, tokenizer, weights)
EMBED_MODEL_DIR=/models/embeddings/bge-small-en-v1.5

# Keep this name in sync with the TEI model you mounted (for consistency & local fallback)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5

# Embedding dimension (match your model: bge-small=384, bge-large=1024, MiniLM=384)
EMBEDDING_DIM=384

# Optional HTTP client parameters
EMBEDDINGS_HTTP_TIMEOUT=60
EMBEDDINGS_MAX_CLIENT_BATCH_SIZE=32
